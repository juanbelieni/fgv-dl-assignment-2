{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ssIFK7m1cOg"
      },
      "source": [
        "# **Homework 2 - Semantic Segmentation**\n",
        "\n",
        "Objective: Implement a U-Net Network for semantic segmentation.\n",
        "\n",
        "\n",
        "\n",
        "Dataset:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://raw.githubusercontent.com/mabelortega/DL_Semantic_Segmentation/main/Figures/drawing1.png'/>\n",
        "</figure>\n",
        "\n",
        "You must train the network model by using these images [https://drive.google.com/file/d/1TU2nTVGS2932hRs1u-ma4r3vmgqHRbMO/view?usp=sharing]. Image_Train.tif and Reference_Train.tif images and it must be evaluated on Image_Test.tif and Reference_Test.tif images. You can use this notebook that contains some basic functions.\n",
        "\n",
        "Experimental Protocol\n",
        "\n",
        "Load the input data\n",
        "1.     Load the images provided from 2D Semantic Labeling-Vaihingen dataset using the function load_tiff_image(image) and normalize the data into the range [0,1] using the function normalization (image)\n",
        "\n",
        "Train the FCN model\n",
        "2.     To train the FCN model you need patches as input. You must extract patches of size w-by-w-by-c pixels from Image_Train and patches with size w-by-w from Reference_Train. The number of patches and the w must be chosen based on the input size of network.\n",
        "\n",
        "3.     Split randomly the training patches into two sets: Training (80%) and validation (20%).\n",
        "\n",
        "4.     Convert the patches of the Reference image into one-hot encoding base on the number of classes. Hint: Use the function tf.keras.utils.to_categorical.\n",
        "\n",
        "5.     Create the function of the U-Net model - Using skip connections: Hint: use tensorflow.keras.layers.concatenate\n",
        "\n",
        "6.     For training, use the weighted_categorical_crossentropy as a loss function. Hint: To compute the weights you must count the number of pixels of each class and apply the formula: w_i = #total_pixels / #pixels_of_class_i\n",
        "\n",
        "\n",
        "\n",
        "7.     Train the model using Train_model() function, which has as input the training and validation patches. You must the best model adding the early stop strategy with patience equal to 10.\n",
        "\n",
        "8.     Extract patches from the test images and test the model using Test(model, patch_test).\n",
        "\n",
        "9.     Reconstruct the prediction (whole test image)\n",
        "\n",
        "The report must present the classification results as label images, and report accuracy metrics (overall and average class accuracies, F1-score) you also must change the size of the extracted patches to compare the results (32x32, 64x64, 128x128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGhOgDtM9W8c"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xE6yP1WF9KlA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-15 17:19:03.675159: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-09-15 17:19:03.785017: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-09-15 17:19:03.786576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-15 17:19:06.434177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import keras as K\n",
        "from PIL import Image\n",
        "from sklearn.utils import shuffle\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZb1JUAHcUm"
      },
      "source": [
        "# Load images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_tiff_image(path, grayscale=False):\n",
        "    image = Image.open(path)\n",
        "    image = image.convert(\"L\") if grayscale else image\n",
        "\n",
        "    return np.array(image)\n",
        "\n",
        "\n",
        "train_image = load_tiff_image(\"images/Images_Train/Image_Train.tif\")\n",
        "train_reference = load_tiff_image(\n",
        "    \"images/Images_Train/Reference_Train.tif\",\n",
        "    grayscale=True,\n",
        ")\n",
        "\n",
        "test_image = load_tiff_image(\"images/Images_Test/Image_Test.tif\")\n",
        "test_reference = load_tiff_image(\n",
        "    \"images/Images_Test/Reference_Test.tif\",\n",
        "    grayscale=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot_encode(image: np.ndarray):\n",
        "    W, H = image.shape\n",
        "    \n",
        "    colors = np.sort(np.unique(image))\n",
        "    image_encoded = np.zeros((W, H, len(colors)))\n",
        "\n",
        "    for i, color in enumerate(colors):\n",
        "        image_encoded[:, :, i] = image == color\n",
        "\n",
        "    return image_encoded\n",
        "\n",
        "train_reference_encoded = one_hot_encode(train_reference)\n",
        "test_reference_encoded = one_hot_encode(test_reference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalization(image: np.ndarray):\n",
        "    W, H, C = image.shape\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    image_normalized = scaler.fit_transform(image.reshape((W * H), C))\n",
        "    image_normalized = image_normalized.reshape(W, H, C)\n",
        "\n",
        "    return image_normalized\n",
        "\n",
        "train_image_normalized = normalization(train_image)\n",
        "test_image_normalized = normalization(test_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OckMql719bbO"
      },
      "source": [
        "# **Define the functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8SK7jWmG9Vpn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_patches(image, size, stride):\n",
        "    W, H, _ = image.shape\n",
        "    patches = []\n",
        "\n",
        "    for i in range(0, W, stride):\n",
        "        for j in range(0, H, stride):\n",
        "            if i + size > W or j + size > H:\n",
        "                continue\n",
        "\n",
        "            patch = image[i : i + size, j : j + size]\n",
        "            patches.append(patch)\n",
        "\n",
        "    return np.array(patches).reshape(-1, size, size, 3)\n",
        "\n",
        "\n",
        "def unet(input_shape, n_classes):\n",
        "    pass\n",
        "    # input_img = Input(input_shape)\n",
        "    # # You must complete the U-Net architecture\n",
        "    # output = Conv2D(n_classes, (1, 1), activation=\"softmax\")(merged)\n",
        "    # return Model(inputs=input_img, outputs=output, name=\"U-Net\")\n",
        "\n",
        "\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "\n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "\n",
        "    Usage:\n",
        "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "        loss = weighted_categorical_crossentropy(weights)\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "    weights = K.variable(weights)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        # scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        loss = y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred)\n",
        "        loss = loss * weights\n",
        "        loss = -K.mean(loss, -1)\n",
        "        return loss\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train_model(\n",
        "    net,\n",
        "    patches_train,\n",
        "    patches_tr_lb_h,\n",
        "    patches_val,\n",
        "    patches_val_lb_h,\n",
        "    batch_size,\n",
        "    epochs,\n",
        "):\n",
        "    print(\"Start training.. \")\n",
        "    for epoch in range(epochs):\n",
        "        loss_tr = np.zeros((1, 2))\n",
        "        loss_val = np.zeros((1, 2))\n",
        "        # Computing the number of batchs\n",
        "        n_batchs_tr = patches_train.shape[0] // batch_size\n",
        "        # Random shuffle the data\n",
        "        patches_train, patches_tr_lb_h = shuffle(\n",
        "            patches_train, patches_tr_lb_h, random_state=0\n",
        "        )\n",
        "\n",
        "        # Training the network per batch\n",
        "        for batch in range(n_batchs_tr):\n",
        "            x_train_b = patches_train[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            y_train_h_b = patches_tr_lb_h[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            loss_tr = loss_tr + net.train_on_batch(x_train_b, y_train_h_b)\n",
        "\n",
        "        # Training loss\n",
        "        loss_tr = loss_tr / n_batchs_tr\n",
        "        print(\n",
        "            \"%d [Training loss: %f , Train acc.: %.2f%%]\"\n",
        "            % (epoch, loss_tr[0, 0], 100 * loss_tr[0, 1])\n",
        "        )\n",
        "\n",
        "        # Computing the number of batchs\n",
        "        n_batchs_val = patches_val.shape[0] // batch_size\n",
        "\n",
        "        # Evaluating the model in the validation set\n",
        "        for batch in range(n_batchs_val):\n",
        "            x_val_b = patches_val[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            y_val_h_b = patches_val_lb_h[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            loss_val = loss_val + net.test_on_batch(x_val_b, y_val_h_b)\n",
        "\n",
        "        # validation loss\n",
        "        loss_val = loss_val / n_batchs_val\n",
        "        print(\n",
        "            \"%d [Validation loss: %f , Validation acc.: %.2f%%]\"\n",
        "            % (epoch, loss_val[0, 0], 100 * loss_val[0, 1])\n",
        "        )\n",
        "        # Add early stopping\n",
        "\n",
        "\n",
        "def test(model, patch_test):\n",
        "    result = model.predict(patch_test)\n",
        "    predicted_class = np.argmax(result, axis=-1)\n",
        "    return predicted_class\n",
        "\n",
        "\n",
        "def compute_metrics(true_labels, predicted_labels):\n",
        "    accuracy = 100 * accuracy_score(true_labels, predicted_labels)\n",
        "    f1score = 100 * f1_score(true_labels, predicted_labels, average=None)\n",
        "    recall = 100 * recall_score(true_labels, predicted_labels, average=None)\n",
        "    precision = 100 * precision_score(true_labels, predicted_labels, average=None)\n",
        "    return accuracy, f1score, recall, precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "4osv4Y9DDTm7",
        "outputId": "44f9dd4a-cb67-4a7a-c77e-732be8a63431"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a90a782274fb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extract training patches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpatches_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches_train_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_ref_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# One hot encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpatches_tr_lb_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tr_img' is not defined"
          ]
        }
      ],
      "source": [
        "# # Extract training patches\n",
        "# patches_train, patches_train_ref = extract_patches(tr_img, tr_ref_img, patch_size, stride)\n",
        "\n",
        "# # One hot encoding\n",
        "# patches_tr_lb_h\n",
        "\n",
        "# # Train the model\n",
        "# adam = Adam(lr = 0.0001 , beta_1=0.9)\n",
        "# net = unet((patch_size, patch_size, channels), number_class)\n",
        "# loss = weighted_categorical_crossentropy(weights)\n",
        "# net.summary()\n",
        "# net.compile(loss = loss, optimizer=adam , metrics=['accuracy'])\n",
        "\n",
        "# # load the model\n",
        "# model = load_model(name)\n",
        "\n",
        "# # Test the model\n",
        "# predicted_labels = Test(model, patch_test)\n",
        "\n",
        "# # Metrics\n",
        "# compute_metrics(true_labels, predicted_labels)\n",
        "\n",
        "# # Plot the prediction (whole test image)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
