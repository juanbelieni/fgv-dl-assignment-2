{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ssIFK7m1cOg"
      },
      "source": [
        "# **Homework 2 - Semantic Segmentation**\n",
        "\n",
        "Objective: Implement a U-Net Network for semantic segmentation.\n",
        "\n",
        "\n",
        "\n",
        "Dataset:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://raw.githubusercontent.com/mabelortega/DL_Semantic_Segmentation/main/Figures/drawing1.png'/>\n",
        "</figure>\n",
        "\n",
        "You must train the network model by using these images [https://drive.google.com/file/d/1TU2nTVGS2932hRs1u-ma4r3vmgqHRbMO/view?usp=sharing]. Image_Train.tif and Reference_Train.tif images and it must be evaluated on Image_Test.tif and Reference_Test.tif images. You can use this notebook that contains some basic functions.\n",
        "\n",
        "Experimental Protocol\n",
        "\n",
        "Load the input data\n",
        "1.     Load the images provided from 2D Semantic Labeling-Vaihingen dataset using the function load_tiff_image(image) and normalize the data into the range [0,1] using the function normalization (image)\n",
        "\n",
        "Train the FCN model\n",
        "2.     To train the FCN model you need patches as input. You must extract patches of size w-by-w-by-c pixels from Image_Train and patches with size w-by-w from Reference_Train. The number of patches and the w must be chosen based on the input size of network.\n",
        "\n",
        "3.     Split randomly the training patches into two sets: Training (80%) and validation (20%).\n",
        "\n",
        "4.     Convert the patches of the Reference image into one-hot encoding base on the number of classes. Hint: Use the function tf.keras.utils.to_categorical.\n",
        "\n",
        "5.     Create the function of the U-Net model - Using skip connections: Hint: use tensorflow.keras.layers.concatenate\n",
        "\n",
        "6.     For training, use the weighted_categorical_crossentropy as a loss function. Hint: To compute the weights you must count the number of pixels of each class and apply the formula: w_i = #total_pixels / #pixels_of_class_i\n",
        "\n",
        "\n",
        "\n",
        "7.     Train the model using Train_model() function, which has as input the training and validation patches. You must the best model adding the early stop strategy with patience equal to 10.\n",
        "\n",
        "8.     Extract patches from the test images and test the model using Test(model, patch_test).\n",
        "\n",
        "9.     Reconstruct the prediction (whole test image)\n",
        "\n",
        "The report must present the classification results as label images, and report accuracy metrics (overall and average class accuracies, F1-score) you also must change the size of the extracted patches to compare the results (32x32, 64x64, 128x128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGhOgDtM9W8c"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xE6yP1WF9KlA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-25 11:48:48.126843: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-09-25 11:48:48.435534: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-09-25 11:48:48.436776: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-25 11:48:50.338228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import Model\n",
        "from keras.layers import Conv2D, Input, MaxPool2D, UpSampling2D, Conv2DTranspose, concatenate\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.utils import shuffle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZb1JUAHcUm"
      },
      "source": [
        "# Load images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_tiff_image(path, grayscale=False):\n",
        "    image = Image.open(path)\n",
        "    image = image.convert(\"L\") if grayscale else image\n",
        "\n",
        "    return np.array(image)\n",
        "\n",
        "\n",
        "train_image = load_tiff_image(\"images/Images_Train/Image_Train.tif\")\n",
        "train_reference = load_tiff_image(\n",
        "    \"images/Images_Train/Reference_Train.tif\",\n",
        "    grayscale=True,\n",
        ")\n",
        "\n",
        "test_image = load_tiff_image(\"images/Images_Test/Image_Test.tif\")\n",
        "test_reference = load_tiff_image(\n",
        "    \"images/Images_Test/Reference_Test.tif\",\n",
        "    grayscale=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot_encode(image: np.ndarray):\n",
        "    W, H = image.shape\n",
        "    \n",
        "    colors = np.sort(np.unique(image))\n",
        "    image_encoded = np.zeros((W, H, len(colors)))\n",
        "\n",
        "    for i, color in enumerate(colors):\n",
        "        image_encoded[:, :, i] = image == color\n",
        "\n",
        "    return image_encoded\n",
        "\n",
        "train_reference_encoded = one_hot_encode(train_reference)\n",
        "test_reference_encoded = one_hot_encode(test_reference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalization(image: np.ndarray):\n",
        "    W, H, C = image.shape\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    image_normalized = scaler.fit_transform(image.reshape((W * H), C))\n",
        "    image_normalized = image_normalized.reshape(W, H, C)\n",
        "\n",
        "    return image_normalized\n",
        "\n",
        "train_image_normalized = normalization(train_image)\n",
        "test_image_normalized = normalization(test_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OckMql719bbO"
      },
      "source": [
        "# **Define the functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8SK7jWmG9Vpn"
      },
      "outputs": [],
      "source": [
        "def extract_patches(image, size, stride):\n",
        "    W, H, C = image.shape\n",
        "    patches = []\n",
        "\n",
        "    for i in range(0, W, stride):\n",
        "        for j in range(0, H, stride):\n",
        "            if i + size > W or j + size > H:\n",
        "                continue\n",
        "\n",
        "            patch = image[i : i + size, j : j + size]\n",
        "            patches.append(patch)\n",
        "\n",
        "    return np.array(patches).reshape(-1, size, size, C)\n",
        "\n",
        "\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    weights = K.variable(weights)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "\n",
        "        loss = y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred)\n",
        "        loss = -K.mean(loss * weights, -1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convolution_layers(layer, n_filters):\n",
        "    conv = Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(layer)\n",
        "    conv = Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(conv)\n",
        "    return conv\n",
        "\n",
        "\n",
        "def downsample_block(layer, n_filters):\n",
        "    conv = convolution_layers(layer, n_filters)\n",
        "    pool = MaxPool2D(2)(conv)\n",
        "    return conv, pool\n",
        "\n",
        "\n",
        "def upsample_block(layer, conv_features, n_filters):\n",
        "    layer = Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(layer)\n",
        "    layer = concatenate([layer, conv_features])\n",
        "    layer = convolution_layers(layer, n_filters)\n",
        "    return layer\n",
        "\n",
        "\n",
        "def unet(input_shape, n_classes):\n",
        "    print(input_shape)\n",
        "    in_layer = Input(input_shape)\n",
        "\n",
        "    down_conv1, down_pool1 = downsample_block(in_layer, 64)\n",
        "    down_conv2, down_pool2 = downsample_block(down_conv1, 128)\n",
        "    down_conv3, down_pool3 = downsample_block(down_conv2, 256)\n",
        "    down_conv4, down_pool4 = downsample_block(down_conv3, 512)\n",
        "\n",
        "    middle_conv = convolution_layers(down_conv4, 1024)\n",
        "\n",
        "    up_conv1 = upsample_block(middle_conv, down_conv4, 512)\n",
        "    up_conv2 = upsample_block(up_conv1, down_conv3, 256)\n",
        "    up_conv3 = upsample_block(up_conv2, down_conv2, 128)\n",
        "    up_conv4 = upsample_block(up_conv3, down_conv1, 64)\n",
        "\n",
        "    out_layer = Conv2D(3, 1, padding=\"same\", activation=\"softmax\")(up_conv4)\n",
        "\n",
        "    return Model(in_layer, out_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    patches_train,\n",
        "    patches_tr_lb_h,\n",
        "    patches_val,\n",
        "    patches_val_lb_h,\n",
        "    batch_size,\n",
        "    epochs,\n",
        "):\n",
        "    print(\"Start training.. \")\n",
        "    for epoch in range(epochs):\n",
        "        loss_train = np.zeros((1, 2))\n",
        "        loss_val = np.zeros((1, 2))\n",
        "\n",
        "        # Computing the number of batchs\n",
        "        n_batches_train = patches_train.shape[0] // batch_size\n",
        "\n",
        "        # Random shuffle the data\n",
        "        patches_train, patches_tr_lb_h = shuffle(\n",
        "            patches_train, patches_tr_lb_h, random_state=0\n",
        "        )\n",
        "\n",
        "        # Training the network per batch\n",
        "        for batch in range(n_batches_train):\n",
        "            x_train_b = patches_train[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            y_train_h_b = patches_tr_lb_h[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            loss_train = loss_train + model.train_on_batch(x_train_b, y_train_h_b)\n",
        "\n",
        "        # Training loss\n",
        "        loss_train = loss_train / n_batches_train\n",
        "        print(\n",
        "            \"%d [Training loss: %f , Train acc.: %.2f%%]\"\n",
        "            % (epoch, loss_train[0, 0], 100 * loss_train[0, 1])\n",
        "        )\n",
        "\n",
        "        # Computing the number of batchs\n",
        "        n_batches_val = patches_val.shape[0] // batch_size\n",
        "\n",
        "        # Evaluating the model in the validation set\n",
        "        for batch in range(n_batches_val):\n",
        "            x_val_b = patches_val[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            y_val_h_b = patches_val_lb_h[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            loss_val = loss_val + model.test_on_batch(x_val_b, y_val_h_b)\n",
        "\n",
        "        # validation loss\n",
        "        loss_val = loss_val / n_batches_val\n",
        "        print(\n",
        "            \"%d [Validation loss: %f , Validation acc.: %.2f%%]\"\n",
        "            % (epoch, loss_val[0, 0], 100 * loss_val[0, 1])\n",
        "        )\n",
        "        # Add early stopping\n",
        "\n",
        "\n",
        "def test(model, patch_test):\n",
        "    result = model.predict(patch_test)\n",
        "    predicted_class = np.argmax(result, axis=-1)\n",
        "    return predicted_class\n",
        "\n",
        "\n",
        "def compute_metrics(true_labels, predicted_labels):\n",
        "    accuracy = 100 * accuracy_score(true_labels, predicted_labels)\n",
        "    f1score = 100 * f1_score(true_labels, predicted_labels, average=None)\n",
        "    recall = 100 * recall_score(true_labels, predicted_labels, average=None)\n",
        "    precision = 100 * precision_score(true_labels, predicted_labels, average=None)\n",
        "    return accuracy, f1score, recall, precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape:   (2565, 1919, 3)\n",
            "Test set shape:       (2558, 2818, 3)\n",
            "Patch sizes:          [32, 64, 128]\n",
            "Validation split:     0.2\n",
            "Number of classes:    5\n",
            "Number of channels:   3\n",
            "Weights:              [0.17625794 0.3359289  0.25502094 0.00267358 0.23011864]\n"
          ]
        }
      ],
      "source": [
        "PATCH_SIZES = [32, 64, 128]\n",
        "VALIDATION_SPLIT = 0.2\n",
        "N_CHANNELS = train_image_normalized.shape[-1]\n",
        "N_CLASSES = train_reference_encoded.shape[-1]\n",
        "\n",
        "# weights are computed as the inverse of the frequency of each class in the training set\n",
        "WEIGHTS = np.array(\n",
        "    [\n",
        "        np.sum(train_reference_encoded[:, :, i] == 1)\n",
        "        for i in range(train_reference_encoded.shape[-1])\n",
        "    ]\n",
        ") / np.sum(train_reference_encoded == 1)\n",
        "\n",
        "print(\"Training set shape:  \", train_image_normalized.shape)\n",
        "print(\"Test set shape:      \", test_image_normalized.shape)\n",
        "print(\"Patch sizes:         \", PATCH_SIZES)\n",
        "print(\"Validation split:    \", VALIDATION_SPLIT)\n",
        "print(\"Number of classes:   \", N_CLASSES)\n",
        "print(\"Number of channels:  \", N_CHANNELS)\n",
        "print(\"Weights:             \", WEIGHTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "4osv4Y9DDTm7",
        "outputId": "44f9dd4a-cb67-4a7a-c77e-732be8a63431"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 64, 64, 512), (None, 32, 32, 512)]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# # Train the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m adam \u001b[39m=\u001b[39m Adam(lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, beta_1\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model \u001b[39m=\u001b[39m unet((size, size, N_CHANNELS), N_CLASSES)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m loss \u001b[39m=\u001b[39m weighted_categorical_crossentropy(WEIGHTS)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
            "\u001b[1;32m/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m down_conv4, down_pool4 \u001b[39m=\u001b[39m downsample_block(down_conv3, \u001b[39m512\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m middle_conv \u001b[39m=\u001b[39m convolution_layers(down_conv4, \u001b[39m1024\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m up_conv1 \u001b[39m=\u001b[39m upsample_block(middle_conv, down_conv4, \u001b[39m512\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m up_conv2 \u001b[39m=\u001b[39m upsample_block(up_conv1, down_conv3, \u001b[39m256\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m up_conv3 \u001b[39m=\u001b[39m upsample_block(up_conv2, down_conv2, \u001b[39m128\u001b[39m)\n",
            "\u001b[1;32m/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupsample_block\u001b[39m(layer, conv_features, n_filters):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     layer \u001b[39m=\u001b[39m Conv2DTranspose(n_filters, \u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msame\u001b[39m\u001b[39m\"\u001b[39m)(layer)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     layer \u001b[39m=\u001b[39m concatenate([layer, conv_features])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     layer \u001b[39m=\u001b[39m convolution_layers(layer, n_filters)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/keras/src/layers/merging/concatenate.py:231\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.layers.concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcatenate\u001b[39m(inputs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    201\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Functional interface to the `Concatenate` layer.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[39m    >>> x = np.arange(20).reshape(2, 2, 5)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m        A tensor, the concatenation of the inputs alongside axis `axis`.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m Concatenate(axis\u001b[39m=\u001b[39;49maxis, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)(inputs)\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/keras/src/layers/merging/concatenate.py:131\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    125\u001b[0m unique_dims \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[1;32m    126\u001b[0m     shape[axis]\n\u001b[1;32m    127\u001b[0m     \u001b[39mfor\u001b[39;00m shape \u001b[39min\u001b[39;00m shape_set\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m shape[axis] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unique_dims) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err_msg)\n",
            "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 64, 64, 512), (None, 32, 32, 512)]"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "for size in PATCH_SIZES:\n",
        "    # # Extract training patches\n",
        "    train_image_patches = extract_patches(train_image_normalized, size, size)\n",
        "    train_reference_patches = extract_patches(train_reference_encoded, size, size)\n",
        "\n",
        "    # Split the training patches into training and validation sets\n",
        "    train_image_patches, train_reference_patches = shuffle(\n",
        "        train_image_patches, train_reference_patches, random_state=0\n",
        "    )\n",
        "\n",
        "    train_size = int(train_image_patches.shape[0] * (1 - VALIDATION_SPLIT))\n",
        "    validation_size = int(train_image_patches.shape[0] * VALIDATION_SPLIT)\n",
        "\n",
        "    train_image_patches, validation_image_patches = (\n",
        "        train_image_patches[:train_size],\n",
        "        train_image_patches[train_size:],\n",
        "    )\n",
        "\n",
        "    train_reference_patches, validation_reference_patches = (\n",
        "        train_reference_patches[:train_size],\n",
        "        train_reference_patches[train_size:],\n",
        "    )\n",
        "\n",
        "    # # Train the model\n",
        "    adam = Adam(lr=0.0001, beta_1=0.9)\n",
        "    model = unet((size, size, N_CHANNELS), N_CLASSES)\n",
        "    loss = weighted_categorical_crossentropy(WEIGHTS)\n",
        "    model.summary()\n",
        "    model.compile(loss=loss, optimizer=adam, metrics=[\"accuracy\"])\n",
        "\n",
        "    train_model(\n",
        "        model,\n",
        "        train_image_patches,\n",
        "        train_reference_patches,\n",
        "        validation_image_patches,\n",
        "        validation_reference_patches,\n",
        "        batch_size=32,\n",
        "        epochs=100,\n",
        "    )\n",
        "\n",
        "    # # load the model\n",
        "    # model = load_model(name)\n",
        "\n",
        "    # Train the model\n",
        "\n",
        "    # # Test the model\n",
        "    # predicted_labels = test(model, patch_test)\n",
        "\n",
        "    # # Metrics\n",
        "    # compute_metrics(true_labels, predicted_labels)\n",
        "\n",
        "    # # Plot the prediction (whole test image)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
