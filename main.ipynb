{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ssIFK7m1cOg"
      },
      "source": [
        "# **Homework 2 - Semantic Segmentation**\n",
        "\n",
        "Objective: Implement a U-Net Network for semantic segmentation.\n",
        "\n",
        "\n",
        "\n",
        "Dataset:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src= 'https://raw.githubusercontent.com/mabelortega/DL_Semantic_Segmentation/main/Figures/drawing1.png'/>\n",
        "</figure>\n",
        "\n",
        "You must train the network model by using these images [https://drive.google.com/file/d/1TU2nTVGS2932hRs1u-ma4r3vmgqHRbMO/view?usp=sharing]. Image_Train.tif and Reference_Train.tif images and it must be evaluated on Image_Test.tif and Reference_Test.tif images. You can use this notebook that contains some basic functions.\n",
        "\n",
        "Experimental Protocol\n",
        "\n",
        "Load the input data\n",
        "1.     Load the images provided from 2D Semantic Labeling-Vaihingen dataset using the function load_tiff_image(image) and normalize the data into the range [0,1] using the function normalization (image)\n",
        "\n",
        "Train the FCN model\n",
        "2.     To train the FCN model you need patches as input. You must extract patches of size w-by-w-by-c pixels from Image_Train and patches with size w-by-w from Reference_Train. The number of patches and the w must be chosen based on the input size of network.\n",
        "\n",
        "3.     Split randomly the training patches into two sets: Training (80%) and validation (20%).\n",
        "\n",
        "4.     Convert the patches of the Reference image into one-hot encoding base on the number of classes. Hint: Use the function tf.keras.utils.to_categorical.\n",
        "\n",
        "5.     Create the function of the U-Net model - Using skip connections: Hint: use tensorflow.keras.layers.concatenate\n",
        "\n",
        "6.     For training, use the weighted_categorical_crossentropy as a loss function. Hint: To compute the weights you must count the number of pixels of each class and apply the formula: w_i = #total_pixels / #pixels_of_class_i\n",
        "\n",
        "\n",
        "\n",
        "7.     Train the model using Train_model() function, which has as input the training and validation patches. You must the best model adding the early stop strategy with patience equal to 10.\n",
        "\n",
        "8.     Extract patches from the test images and test the model using Test(model, patch_test).\n",
        "\n",
        "9.     Reconstruct the prediction (whole test image)\n",
        "\n",
        "The report must present the classification results as label images, and report accuracy metrics (overall and average class accuracies, F1-score) you also must change the size of the extracted patches to compare the results (32x32, 64x64, 128x128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGhOgDtM9W8c"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xE6yP1WF9KlA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-19 08:13:42.184185: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-09-19 08:13:42.552913: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-09-19 08:13:42.554500: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-19 08:13:44.845418: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import Model\n",
        "from keras.layers import Conv2D, Input, MaxPool2D, UpSampling2D, Conv2DTranspose, concatenate\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.utils import shuffle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZb1JUAHcUm"
      },
      "source": [
        "# Load images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_tiff_image(path, grayscale=False):\n",
        "    image = Image.open(path)\n",
        "    image = image.convert(\"L\") if grayscale else image\n",
        "\n",
        "    return np.array(image)\n",
        "\n",
        "\n",
        "train_image = load_tiff_image(\"images/Images_Train/Image_Train.tif\")\n",
        "train_reference = load_tiff_image(\n",
        "    \"images/Images_Train/Reference_Train.tif\",\n",
        "    grayscale=True,\n",
        ")\n",
        "\n",
        "test_image = load_tiff_image(\"images/Images_Test/Image_Test.tif\")\n",
        "test_reference = load_tiff_image(\n",
        "    \"images/Images_Test/Reference_Test.tif\",\n",
        "    grayscale=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot_encode(image: np.ndarray):\n",
        "    W, H = image.shape\n",
        "    \n",
        "    colors = np.sort(np.unique(image))\n",
        "    image_encoded = np.zeros((W, H, len(colors)))\n",
        "\n",
        "    for i, color in enumerate(colors):\n",
        "        image_encoded[:, :, i] = image == color\n",
        "\n",
        "    return image_encoded\n",
        "\n",
        "train_reference_encoded = one_hot_encode(train_reference)\n",
        "test_reference_encoded = one_hot_encode(test_reference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalization(image: np.ndarray):\n",
        "    W, H, C = image.shape\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    image_normalized = scaler.fit_transform(image.reshape((W * H), C))\n",
        "    image_normalized = image_normalized.reshape(W, H, C)\n",
        "\n",
        "    return image_normalized\n",
        "\n",
        "train_image_normalized = normalization(train_image)\n",
        "test_image_normalized = normalization(test_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OckMql719bbO"
      },
      "source": [
        "# **Define the functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8SK7jWmG9Vpn"
      },
      "outputs": [],
      "source": [
        "def extract_patches(image, size, stride):\n",
        "    W, H, C = image.shape\n",
        "    patches = []\n",
        "\n",
        "    for i in range(0, W, stride):\n",
        "        for j in range(0, H, stride):\n",
        "            if i + size > W or j + size > H:\n",
        "                continue\n",
        "\n",
        "            patch = image[i : i + size, j : j + size]\n",
        "            patches.append(patch)\n",
        "\n",
        "    return np.array(patches).reshape(-1, size, size, C)\n",
        "\n",
        "\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    weights = K.variable(weights)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "\n",
        "        loss = y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred)\n",
        "        loss = -K.mean(loss * weights, -1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convolution_layers(input, n_filters):\n",
        "   input = Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\")(input)\n",
        "   input = Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\")(input)\n",
        "   return input\n",
        "\n",
        "def downsample_block(input, n_filters):\n",
        "   f = convolution_layers(input, n_filters)\n",
        "   p = MaxPool2D(2)(f)\n",
        "   return f, p\n",
        "\n",
        "def upsample_block(x, conv_features, n_filters):\n",
        "   x = Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x) # TROCAR AAAAA\n",
        "   x = concatenate([x, conv_features])\n",
        "   x = convolution_layers(x, n_filters)\n",
        "   return x\n",
        "\n",
        "def unet(input_shape, n_classes):\n",
        "    conv_args = {\n",
        "        \"kernel_size\": (3, 3),\n",
        "        \"activation\": \"relu\",\n",
        "        \"padding\": \"same\",\n",
        "    }\n",
        "\n",
        "    input_img = Input(input_shape)\n",
        "    n_filters = [32, 64, 128, 256]\n",
        "\n",
        "    # TODO: Completar\n",
        "\n",
        "    output = None\n",
        "\n",
        "    return Model(input_img, output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    patches_train,\n",
        "    patches_tr_lb_h,\n",
        "    patches_val,\n",
        "    patches_val_lb_h,\n",
        "    batch_size,\n",
        "    epochs,\n",
        "):\n",
        "    print(\"Start training.. \")\n",
        "    for epoch in range(epochs):\n",
        "        loss_train = np.zeros((1, 2))\n",
        "        loss_val = np.zeros((1, 2))\n",
        "\n",
        "        # Computing the number of batchs\n",
        "        n_batches_train = patches_train.shape[0] // batch_size\n",
        "\n",
        "        # Random shuffle the data\n",
        "        patches_train, patches_tr_lb_h = shuffle(\n",
        "            patches_train, patches_tr_lb_h, random_state=0\n",
        "        )\n",
        "\n",
        "        # Training the network per batch\n",
        "        for batch in range(n_batches_train):\n",
        "            x_train_b = patches_train[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            y_train_h_b = patches_tr_lb_h[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            loss_train = loss_train + model.train_on_batch(x_train_b, y_train_h_b)\n",
        "\n",
        "        # Training loss\n",
        "        loss_train = loss_train / n_batches_train\n",
        "        print(\n",
        "            \"%d [Training loss: %f , Train acc.: %.2f%%]\"\n",
        "            % (epoch, loss_train[0, 0], 100 * loss_train[0, 1])\n",
        "        )\n",
        "\n",
        "        # Computing the number of batchs\n",
        "        n_batches_val = patches_val.shape[0] // batch_size\n",
        "\n",
        "        # Evaluating the model in the validation set\n",
        "        for batch in range(n_batches_val):\n",
        "            x_val_b = patches_val[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            y_val_h_b = patches_val_lb_h[\n",
        "                batch * batch_size : (batch + 1) * batch_size, :, :, :\n",
        "            ]\n",
        "            loss_val = loss_val + model.test_on_batch(x_val_b, y_val_h_b)\n",
        "\n",
        "        # validation loss\n",
        "        loss_val = loss_val / n_batches_val\n",
        "        print(\n",
        "            \"%d [Validation loss: %f , Validation acc.: %.2f%%]\"\n",
        "            % (epoch, loss_val[0, 0], 100 * loss_val[0, 1])\n",
        "        )\n",
        "        # Add early stopping\n",
        "\n",
        "\n",
        "def test(model, patch_test):\n",
        "    result = model.predict(patch_test)\n",
        "    predicted_class = np.argmax(result, axis=-1)\n",
        "    return predicted_class\n",
        "\n",
        "\n",
        "def compute_metrics(true_labels, predicted_labels):\n",
        "    accuracy = 100 * accuracy_score(true_labels, predicted_labels)\n",
        "    f1score = 100 * f1_score(true_labels, predicted_labels, average=None)\n",
        "    recall = 100 * recall_score(true_labels, predicted_labels, average=None)\n",
        "    precision = 100 * precision_score(true_labels, predicted_labels, average=None)\n",
        "    return accuracy, f1score, recall, precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape:   (2565, 1919, 3)\n",
            "Test set shape:       (2558, 2818, 3)\n",
            "Patch sizes:          [32, 64, 128]\n",
            "Validation split:     0.2\n",
            "Number of classes:    5\n",
            "Number of channels:   3\n",
            "Weights:              [0.17625794 0.3359289  0.25502094 0.00267358 0.23011864]\n"
          ]
        }
      ],
      "source": [
        "PATCH_SIZES = [32, 64, 128]\n",
        "VALIDATION_SPLIT = 0.2\n",
        "N_CHANNELS = train_image_normalized.shape[-1]\n",
        "N_CLASSES = train_reference_encoded.shape[-1]\n",
        "\n",
        "# weights are computed as the inverse of the frequency of each class in the training set\n",
        "WEIGHTS = np.array(\n",
        "    [\n",
        "        np.sum(train_reference_encoded[:, :, i] == 1)\n",
        "        for i in range(train_reference_encoded.shape[-1])\n",
        "    ]\n",
        ") / np.sum(train_reference_encoded == 1)\n",
        "\n",
        "print(\"Training set shape:  \", train_image_normalized.shape)\n",
        "print(\"Test set shape:      \", test_image_normalized.shape)\n",
        "print(\"Patch sizes:         \", PATCH_SIZES)\n",
        "print(\"Validation split:    \", VALIDATION_SPLIT)\n",
        "print(\"Number of classes:   \", N_CLASSES)\n",
        "print(\"Number of channels:  \", N_CHANNELS)\n",
        "print(\"Weights:             \", WEIGHTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "4osv4Y9DDTm7",
        "outputId": "44f9dd4a-cb67-4a7a-c77e-732be8a63431"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-19 08:18:20.659182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-19 08:18:20.861758: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv1 (Conv2D)              (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " MaxPooling1 (MaxPooling2D)  (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " MaxPooling2 (MaxPooling2D)  (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv3 (Conv2D)              (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " MaxPooling3 (MaxPooling2D)  (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv4 (Conv2D)              (None, 4, 4, 256)         295168    \n",
            "                                                                 \n",
            " upsampling1 (UpSampling2D)  (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 8, 8, 128)         295040    \n",
            "                                                                 \n",
            " upsampling2 (UpSampling2D)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 64)        73792     \n",
            "                                                                 \n",
            " upsampling3 (UpSampling2D)  (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 32, 32, 32)        18464     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 5)         165       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 775877 (2.96 MB)\n",
            "Trainable params: 775877 (2.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Start training.. \n",
            "0 [Training loss: 0.093755 , Train acc.: 46.70%]\n",
            "0 [Validation loss: 0.099027 , Validation acc.: 37.44%]\n",
            "1 [Training loss: 0.079065 , Train acc.: 56.18%]\n",
            "1 [Validation loss: 0.073079 , Validation acc.: 65.50%]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mloss, optimizer\u001b[39m=\u001b[39madam, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m train_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     train_image_patches,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     train_reference_patches,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     validation_image_patches,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     validation_reference_patches,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# # load the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# model = load_model(name)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# # Plot the prediction (whole test image)\u001b[39;00m\n",
            "\u001b[1;32m/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     x_train_b \u001b[39m=\u001b[39m patches_train[\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         batch \u001b[39m*\u001b[39m batch_size : (batch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m batch_size, :, :, :\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     y_train_h_b \u001b[39m=\u001b[39m patches_tr_lb_h[\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         batch \u001b[39m*\u001b[39m batch_size : (batch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m batch_size, :, :, :\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     ]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     loss_train \u001b[39m=\u001b[39m loss_train \u001b[39m+\u001b[39m model\u001b[39m.\u001b[39;49mtrain_on_batch(x_train_b, y_train_h_b)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Training loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juan/dev/juanbelieni/fgv-dl-assignment-2/main.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss_train \u001b[39m=\u001b[39m loss_train \u001b[39m/\u001b[39m n_batches_train\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/keras/src/engine/training.py:2680\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2676\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m   2677\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m   2678\u001b[0m     \u001b[39mself\u001b[39m\n\u001b[1;32m   2679\u001b[0m ):\n\u001b[0;32m-> 2680\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49msingle_batch_iterator(\n\u001b[1;32m   2681\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2682\u001b[0m     )\n\u001b[1;32m   2683\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[1;32m   2684\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1922\u001b[0m, in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingle_batch_iterator\u001b[39m(\n\u001b[1;32m   1919\u001b[0m     strategy, x, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, class_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m ):\n\u001b[1;32m   1921\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Creates a single-batch dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1922\u001b[0m     x, y, sample_weight \u001b[39m=\u001b[39m _process_tensorlike((x, y, sample_weight))\n\u001b[1;32m   1923\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1924\u001b[0m         data \u001b[39m=\u001b[39m (x,)\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1163\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[39mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[1;32m   1161\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m-> 1163\u001b[0m inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_convert_single_tensor, inputs)\n\u001b[1;32m   1164\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mlist_to_tuple(inputs)\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/util/nest.py:624\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnest.map_structure\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    539\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_structure\u001b[39m(func, \u001b[39m*\u001b[39mstructure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    540\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[39m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m   \u001b[39mreturn\u001b[39;00m nest_util\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[1;32m    625\u001b[0m       nest_util\u001b[39m.\u001b[39;49mModality\u001b[39m.\u001b[39;49mCORE, func, \u001b[39m*\u001b[39;49mstructure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    626\u001b[0m   )\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1054\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \n\u001b[1;32m    959\u001b[0m \u001b[39m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[39m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39mif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mCORE:\n\u001b[0;32m-> 1054\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_core_map_structure(func, \u001b[39m*\u001b[39;49mstructure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1055\u001b[0m \u001b[39melif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mDATA:\n\u001b[1;32m   1056\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[39m*\u001b[39mstructure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[39m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[39mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[39m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[39m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[39mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[39m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1158\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_single_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, np\u001b[39m.\u001b[39mfloating):\n\u001b[1;32m   1157\u001b[0m         dtype \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mfloatx()\n\u001b[0;32m-> 1158\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(x, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m   1159\u001b[0m \u001b[39melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[1;32m   1160\u001b[0m     \u001b[39mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:160\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39m@tf_export\u001b[39m\u001b[39m.\u001b[39mtf_export(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m     96\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m     98\u001b[0m     value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     99\u001b[0m ):\n\u001b[1;32m    100\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[39m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor_v2(\n\u001b[1;32m    161\u001b[0m       value, dtype\u001b[39m=\u001b[39;49mdtype, dtype_hint\u001b[39m=\u001b[39;49mdtype_hint, name\u001b[39m=\u001b[39;49mname\n\u001b[1;32m    162\u001b[0m   )\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:168\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_conversion_registry\u001b[39m.\u001b[39;49mconvert(\n\u001b[1;32m    169\u001b[0m     value, dtype, name, preferred_dtype\u001b[39m=\u001b[39;49mdtype_hint\n\u001b[1;32m    170\u001b[0m )\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:324\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    323\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 324\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:263\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    168\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    264\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:275\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    274\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 275\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    277\u001b[0m const_tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39m_create_graph_constant(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m const_tensor\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:285\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    284\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    286\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.11.0/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:86\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m     83\u001b[0m   \u001b[39m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[1;32m     84\u001b[0m   \u001b[39m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[1;32m     85\u001b[0m   \u001b[39m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m   value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, ops\u001b[39m.\u001b[39mEagerTensor):\n\u001b[1;32m     88\u001b[0m   \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m dtype:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "for size in PATCH_SIZES:\n",
        "    # # Extract training patches\n",
        "    train_image_patches = extract_patches(train_image_normalized, size, size)\n",
        "    train_reference_patches = extract_patches(train_reference_encoded, size, size)\n",
        "\n",
        "    # Split the training patches into training and validation sets\n",
        "    train_image_patches, train_reference_patches = shuffle(\n",
        "        train_image_patches, train_reference_patches, random_state=0\n",
        "    )\n",
        "\n",
        "    train_size = int(train_image_patches.shape[0] * (1 - VALIDATION_SPLIT))\n",
        "    validation_size = int(train_image_patches.shape[0] * VALIDATION_SPLIT)\n",
        "\n",
        "    train_image_patches, validation_image_patches = (\n",
        "        train_image_patches[:train_size],\n",
        "        train_image_patches[train_size:],\n",
        "    )\n",
        "\n",
        "    train_reference_patches, validation_reference_patches = (\n",
        "        train_reference_patches[:train_size],\n",
        "        train_reference_patches[train_size:],\n",
        "    )\n",
        "\n",
        "    # # Train the model\n",
        "    adam = Adam(lr=0.0001, beta_1=0.9)\n",
        "    model = unet((size, size, N_CHANNELS), N_CLASSES)\n",
        "    loss = weighted_categorical_crossentropy(WEIGHTS)\n",
        "    model.summary()\n",
        "    model.compile(loss=loss, optimizer=adam, metrics=[\"accuracy\"])\n",
        "\n",
        "    train_model(\n",
        "        model,\n",
        "        train_image_patches,\n",
        "        train_reference_patches,\n",
        "        validation_image_patches,\n",
        "        validation_reference_patches,\n",
        "        batch_size=32,\n",
        "        epochs=100,\n",
        "    )\n",
        "\n",
        "    # # load the model\n",
        "    # model = load_model(name)\n",
        "\n",
        "    # Train the model\n",
        "\n",
        "    # # Test the model\n",
        "    # predicted_labels = test(model, patch_test)\n",
        "\n",
        "    # # Metrics\n",
        "    # compute_metrics(true_labels, predicted_labels)\n",
        "\n",
        "    # # Plot the prediction (whole test image)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
